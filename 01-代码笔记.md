## 爬虫笔记记录

## 一、基本的爬虫

### 访问百度主页

```python
import requests

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36"}

res = requests.get("http://www.baidu.com", headers = headers)
# print(res.text)
print(res.headers) # 返回请求头
print(res.status_code) # 返回状态码
```

### 爬取百度logo

+ 坑点：Windows下文件目录！！！

```python
import requests
res = requests.get("https://www.baidu.com/img/bd_logo1.png")
with open("new.png", "wb") as f:
    f.write(res.content)
    f.close()
```

### 解决JavaScript渲染问题

+ 分析Ajax请求
+ 调用浏览器渲染Selenium/ WebDriver
+ Splash
+ PyV8、Ghost.py

### 怎么样保存数据

+ 文本方式：纯文本、json、xml等
+ 关系型数据库
+ 非关系型数据库
+ 二进制文件

## 二、Urllib库

Python内置的HTTP请求库，只需要关心请求的参数，不需要关心底层实现

内容包括：

+ urllib.request 请求模块
+ urllib.error 异常处理模块
+ urllib.parse 解析模块
+ urllib.robotparser robots.txt解析模块

函数定义：

```python
def urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,
            *, cafile=None, capath=None, cadefault=False, context=None):
```

+ url
+ data
  + 用于post传参
+ timeout
+ CA证书相关参数

示例代码：

### urlopen参数的使用*

#### get 方式

```python
import urllib.request
import urllib.parse

res = urllib.request.urlopen("http://localhost:3000/db_lab")
# print(res.read()) # 原始二进制数据
print(res.read().decode("utf-8")) # 解析后的 数据
```

#### post方式

```python
import urllib.request
import urllib.parse

data = bytes(urllib.parse.urlencode({'word': 'hello_world'}), encoding='utf=8')
res = urllib.request.urlopen("http://httpbin.org/post", data=data)
print(res.read())
```

#### timeout参数 && 捕获异常

```python
import urllib.request
import urllib.error
import socket

try:
    res = urllib.request.urlopen("http://httpbin.org/get", timeout=0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):
        print("Time Out")
else:
    print(res.read())
```

### HTTPResponse对象属性*

+ type(res)	类型是<class 'http.client.HTTPResponse'>
+ status 状态码
+ getheaders() 获取响应头，元组 组成 的列表
+ getheaders('key') 获取响应头的特定字段
+ read().decode("utf-8") 响应体解码

### urlopen 传入 Request对象

更加方便的指定 一些请求的参数，形成整体的结构

```python
import urllib.request
import urllib.parse

url = "http://httpbin.org/post"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36",
    "Host": "httpbin.org"
}
dict = {
    "name": "Amy"
}
data = bytes(urllib.parse.urlencode(dict), encoding='utf8')
req = urllib.request.Request(url=url, data=data, headers=headers, method="POST")
res = urllib.request.urlopen(req)
print(res.read().decode("utf8"))
```

```json
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "name": "Amy"
  }, 
  "headers": {
    "Accept-Encoding": "identity", 
    "Content-Length": "8", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36"
  }, 
  "json": null, 
  "origin": "211.143.85.88, 211.143.85.88", 
  "url": "https://httpbin.org/post"
}
```

另外添加 headers 的方式

```python
req.add_header('key','value')
```

以上是最基本的HTTP请求，这也对应了 request 的四个重要组成部分：url，方式，请求头，请求体(Form Data)把参数直接传递给Request对象，可以构造出整个 请求作为整体发送给 服务器。这样就可以完成大部分网站的爬取。还有一些高级用法，比如设置Cookie，设置代理这样的操作。需要一些Handler实现功能

查看urllib.request的官方文档

### Handler**

#### 代理

可以伪装自己ip地址，服务器识别IP为代理的IP，爬虫的时候可以切换IP，就有效避免 反扒

```python
import urllib.request

# 构造Handler，根据API把参数传入
# 这里传入代理的网址（XX-net之类的端口，必须存在）
proxy_hander = urllib.request.ProxyHandler({
    'http': 'http://127.0.0.1:8087',
    'https': 'https://127.0.0.1:8087'
})

opener = urllib.request.build_opener(proxy_hander)
res = opener.open('http://www.baidu.com')
```

注：未测试成功

#### Cookie

cookie可以维持登录信息，这样就可以爬取一些需要登录认证的网页

+ 获取cookie并打印出来

```python
import http.cookiejar
import urllib.request

cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
res = opener.open("http://www.baidu.com")
for item in cookie:
    print(item.name, '=', item.value)
```

```
BAIDUID = 5E2096C64DB7A23B6B5961E0DE8F4CD9:FG=1
BIDUPSID = 5E2096C64DB7A23B6B5961E0DE8F4CD9
H_PS_PSSID = 26522_1452_28777_21123_28518_28771_28722_28964_28839_28585_26350_22157
PSTM = 1557836206
delPer = 0
BDSVRTM = 0
BD_HOME = 0
```

+ 把cookie保存到文件 用http.cookiejar.MozillaCookieJar 类对象，有save方法

```python
import http.cookiejar
import urllib.request

filename = "cookie.txt"
cookie = http.cookiejar.MozillaCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
res = opener.open("http://www.baidu.com")
cookie.save(ignore_discard=True, ignore_expires=True)
```

最后txt以特定格式保存了cookie信息

> http.cookiejar.LWPCookieJar也可以

可用load方法读取cookie，再用Handler构建出来，这样再次请求的时候就会把Cookie附着进去

```python
cookie.load("cookie.txt",ignore_discard=True, ignore_expires=True)
```

如果网页需要登录，有了cookie后，请求的结果就是登录后所能看到的

### 异常处理**

#### URLError

```python
from urllib import request
from urllib import error
try:
    res = request.urlopen("http://localhost:3000/pad")
except error.URLError as e:
    print(e.reason)
```

经测试，不同url返回如下：

```
http://localhost:3000/pad  Not Found
http://o.o  [Errno 11001] getaddrinfo failed
http://localhost  [WinError 10061] 由于目标计算机积极拒绝，无法连接。
```

#### HTTPError

先捕捉HTTPError再捕捉URLError是比较好的写法：

```python
from urllib import request
from urllib import error
try:
    res = request.urlopen("http://localhost:3000/f")
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, sep='\n')
except error.URLError as e:
    print(e.reason)
else:
    print("Request Successful!")
```

```
Not Found
404
Date: Tue, 14 May 2019 12:36:34 GMT
Server: Apache/2.4.25 (Win32) OpenSSL/1.0.2j PHP/5.6.30
Content-Length: 199
Connection: close
Content-Type: text/html; charset=iso-8859-1
```

#### 具体验证异常原因的方法

```python
import socket
from urllib import request
from urllib import error
try:
    res = request.urlopen("http://www.baidu.com", timeout=0.001)
except error.URLError as e:
    print(type(e))
    if isinstance(e.reason, socket.timeout):
        print("TIME OUT")
```

```
<class 'urllib.error.URLError'>
TIME OUT
```

### URL解析

#### urlparse

函数定义：

```python
def urlparse(url, scheme='', allow_fragments=True)
```

传入url，把url分割成几个部分，然后每个部分依次赋值

```python
from urllib.parse import urlparse
result = urlparse("https://baike.baidu.com/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91/85895?fr=aladdin#3_2") # <class 'urllib.parse.ParseResult'>
print(type(result), result)
'''
ParseResult(scheme='https', netloc='baike.baidu.com', path='/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91/85895', params='', query='fr=aladdin', fragment='3_2')
'''
```

第二个参数scheme

+ 若URL中有协议类型，此参数失效
+ 若URL中无协议类型，此参数会填充到返回结果

第三个参数allow_fragments

+ False情况 -- 把锚点解析到query或者path中去，不会解析到fragment中
+ True情况 -- 正常情况

#### urlunparse

作用：将URL进行拼接

```python
from urllib.parse import urlunparse

data = ["http", "www.baidu.com","index.html",'user','a=6','comment']
print(urlunparse(data))
```

```
http://www.baidu.com/index.html;user?a=6#comment
```

#### urljoin

作用：用于拼接URL，两个URL拼接成一个新的URL；规则：URL可以分成6个字段，以后边的URL为基准，如果某些字段不存在，会用前边的URL来补充

![1557886678927](./img/urljoin.png)

#### urlencode

作用：把一个字典对象转化为get请求参数

```python
from urllib.parse import urlencode

params = {
    "name": "xiaohuihui",
    "age": 22
}
base_url = "http://www.baidu.com?"
url = base_url + urlencode(params)
print(url)
```

```
http://www.baidu.com?name=xiaohuihui&age=22
```

>urllib中最重要的是urllib.request模块，提供了很多的Handler
>
><https://docs.python.org/3/library/urllib.html>

> urllib库使用中有很多不方便的地方，不如加代理，处理Cookie的时候，操作繁琐，另外发送POST请求也……

## 三、Request库

比urllib库更好用，节约大量工作，完全满足HTTP测试需求 

Python实现的简单易用的HTTP库

https://2.python-requests.org/en/master/

### 实例引入

```python
import requests
res = requests.get("http://59.67.225.73/jiaowuchu")
print(type(res)) # <class 'requests.models.Response'>
print(res.status_code) # 200
print(type(res.text)) # <class 'str'>
print(res.text) # html代码
print(res.cookies) # <RequestsCookieJar[]>
```

+ status_code
+ text
+ cookie

### 各种请求方式

```python
import requests
r1 = requests.get(url="",params="")
r1 = requests.post(url="", data="", json="")
r1 = requests.put(url="",data="")
r1 = requests.delete(url="")
r1 = requests.head(url="")
r1 = requests.options(url="")
```

后边都是不定长关键词参数**kwargs

#### 基本get请求

##### 基本写法

```python
import requests

res = requests.get("http://httpbin.org/get")
print(res.text)
```

##### 带参数的get请求

+ 手动加入?xx=xx
+ 字典形式加入，更方便的构造get请求的URL

```python
import requests

data = {
    "name": "Amy",
    "age": 18
}
res = requests.get("http://httpbin.org/get", params=data)
print(res.text)
```

```json
{
  "args": {
    "age": "18", 
    "name": "Amy"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.21.0"
  }, 
  "origin": "222.222.191.99, 222.222.191.99", 
  "url": "https://httpbin.org/get?name=Amy&age=18"
}
```

##### 解析json

在一些ajax请求中比较常用

```python
import requests
import json

data = {
    "name": "Amy",
    "age": 18
}
res = requests.get("http://httpbin.org/get", params=data)
print(type(res.text)) # <class 'str'>
print(res.text)
print(type(res.json())) # <class 'dict'>
print(json.loads(res.text)) # res.json()是实际效果
```

##### 获取二进制数据content

下载图片、视频等常用的方法

```python
import requests

res = requests.get("http://59.67.225.73/jiaowuchu/images/ncepu60.png")
print(type(res.text), type(res.content))
print(res.text)  # <class 'str'> 一堆乱码
print(res.content)  # <class 'bytes'> 字节流
with open("ncepu60.png", "wb") as f:
    f.write(res.content)
    f.close()
```

##### 添加headers

有些网站会识别User-Agent，否则会被拒绝

```python
import requests

res = requests.get("https://www.zhihu.com/explore")
print(res.text)
```

<html>

<head><title>400 Bad Request</title></head>
<body bgcolor="white">

<center><h1>400 Bad Request</h1></center>
<hr><center>openresty</center>
</body>
</html>

添加User-Agent后，相当于进行了浏览器的伪装，就正常访问了

```python
import requests

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0"
}
res = requests.get("https://www.zhihu.com/explore", headers = headers)
with open("result.html", "w+", encoding="utf-8") as f:
    f.write(res.text)
    f.close()
```

#### POST请求

定义：

```python
def post(url, data=None, json=None, **kwargs)
```

可以另外加headers

```python
import requests

data = {
    'c': "网络1502"
}
res = requests.post("http://localhost:3000/GPA/getdata.php", data=data)
print(res.json())
```

附PHP后台代码：

```php
<?php
define('IN_TG', true);
require dirname(__FILE__).'/includes/common.inc.php';
function getdata($sql){
    $result = mysql_query($sql);
    $json = array();
    while (!!$row =mysql_fetch_array($result,MYSQL_ASSOC)) {
        array_push($json,$row);
    }
    echo json_encode($json);
}
if(isset($_POST['a'])){
    $zyh = $_POST['a'];
    getdata("SELECT sno,GM_BJ.bm,sname,xfj FROM XFJ,GM_BJ WHERE XFJ.BM = GM_BJ.BM AND ZYH = $zyh ORDER BY SNO;");
}
if(isset($_POST['b'])){
    $zyh = $_POST['b'];
    getdata("SELECT bm FROM gm_bj WHERE zyh=$zyh ORDER BY bm");
}
if(isset($_POST['c'])){
    $bm = $_POST['c'];
    getdata("SELECT sno,bm,sname,xfj FROM XFJ WHERE bm = '$bm' ORDER BY SNO");
}
?>
```

#### 响应

##### response属性

+ status | int类型，状态码
+ headers | <class 'requests.structures.CaseInsensitiveDict'> 请求头字典
+ cookies | <class 'requests.cookies.RequestsCookieJar'> 
+ url | <class 'str'>
+ history | <class 'list'> 访问的历史记录
+ encoding、next、ok、reason、

#### 状态码判断

<code>res.status_code ==  request.codes.【关键词】</code>

<code>res.status_code == 状态码</code>

```python
import requests

res = requests.get("http://www.baidu.com")
exit() if not res.status_code == requests.codes.ok else print("Request Successfully")
```

```python
import requests

res = requests.get("http://www.baidu.com")
exit() if not res.status_code == 200 else print("Request Successfully")
```

```python
_codes = {

    # Informational.
    100: ('continue',),
    101: ('switching_protocols',),
    102: ('processing',),
    103: ('checkpoint',),
    122: ('uri_too_long', 'request_uri_too_long'),
    200: ('ok', 'okay', 'all_ok', 'all_okay', 'all_good', '\\o/', '✓'),
    201: ('created',),
    202: ('accepted',),
    203: ('non_authoritative_info', 'non_authoritative_information'),
    204: ('no_content',),
    205: ('reset_content', 'reset'),
    206: ('partial_content', 'partial'),
    207: ('multi_status', 'multiple_status', 'multi_stati', 'multiple_stati'),
    208: ('already_reported',),
    226: ('im_used',),

    # Redirection.
    300: ('multiple_choices',),
    301: ('moved_permanently', 'moved', '\\o-'),
    302: ('found',),
    303: ('see_other', 'other'),
    304: ('not_modified',),
    305: ('use_proxy',),
    306: ('switch_proxy',),
    307: ('temporary_redirect', 'temporary_moved', 'temporary'),
    308: ('permanent_redirect',
          'resume_incomplete', 'resume',),  # These 2 to be removed in 3.0

    # Client Error.
    400: ('bad_request', 'bad'),
    401: ('unauthorized',),
    402: ('payment_required', 'payment'),
    403: ('forbidden',),
    404: ('not_found', '-o-'),
    405: ('method_not_allowed', 'not_allowed'),
    406: ('not_acceptable',),
    407: ('proxy_authentication_required', 'proxy_auth', 'proxy_authentication'),
    408: ('request_timeout', 'timeout'),
    409: ('conflict',),
    410: ('gone',),
    411: ('length_required',),
    412: ('precondition_failed', 'precondition'),
    413: ('request_entity_too_large',),
    414: ('request_uri_too_large',),
    415: ('unsupported_media_type', 'unsupported_media', 'media_type'),
    416: ('requested_range_not_satisfiable', 'requested_range', 'range_not_satisfiable'),
    417: ('expectation_failed',),
    418: ('im_a_teapot', 'teapot', 'i_am_a_teapot'),
    421: ('misdirected_request',),
    422: ('unprocessable_entity', 'unprocessable'),
    423: ('locked',),
    424: ('failed_dependency', 'dependency'),
    425: ('unordered_collection', 'unordered'),
    426: ('upgrade_required', 'upgrade'),
    428: ('precondition_required', 'precondition'),
    429: ('too_many_requests', 'too_many'),
    431: ('header_fields_too_large', 'fields_too_large'),
    444: ('no_response', 'none'),
    449: ('retry_with', 'retry'),
    450: ('blocked_by_windows_parental_controls', 'parental_controls'),
    451: ('unavailable_for_legal_reasons', 'legal_reasons'),
    499: ('client_closed_request',),

    # Server Error.
    500: ('internal_server_error', 'server_error', '/o\\', '✗'),
    501: ('not_implemented',),
    502: ('bad_gateway',),
    503: ('service_unavailable', 'unavailable'),
    504: ('gateway_timeout',),
    505: ('http_version_not_supported', 'http_version'),
    506: ('variant_also_negotiates',),
    507: ('insufficient_storage',),
    509: ('bandwidth_limit_exceeded', 'bandwidth'),
    510: ('not_extended',),
    511: ('network_authentication_required', 'network_auth', 'network_authentication'),
}
```

> 注：requests 会默认处理跳转

### 高级操作

request函数可以携带的参数汇总：

+ url
+ param | get参数
+ headers 
+ data | post参数
+ files | POST文件上传
+ verify | 是否验证
+ cert | 手动导入证书
+ proxies | 代理设置
+ timeout | 超时设置
+ auth | 认证设置
+ allow_redirects | 是否允许自动处理跳转

#### 文件上传

```python
import requests

files = {
    'file0': open("ncepu60.png", "rb"),
    'file1': open("new.png", "rb")
}
res = requests.post("http://httpbin.org/post", files=files)
print(res.text)
```

输出结果：

```python
{
  "args": {}, 
  "data": "", 
  "files": {
    "file0": "data:application/octet-stream;base64,……", 
    "file1": "data:application/octet-stream;base64,……"
  }, 
  "form": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "61023", 
    "Content-Type": "multipart/form-data; boundary=b1ecaa54edacb0ce921c2b6cb01adece", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.21.0"
  }, 
  "json": null, 
  "origin": "222.222.191.99, 222.222.191.99", 
  "url": "https://httpbin.org/post"
}
```

#### 获取cookie

```python
import requests

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/17.17134"
}
res = requests.get("https://www.baidu.com", headers=headers)
print(res.cookies)
print()
for k, v in res.cookies.items():
    print(k, '=', v)

```

#### 会话维持

模拟登陆

+ 不成功的例子

  ```python
  import requests
  
  requests.get("http://httpbin.org/cookies/set/number/123456789")
  res = requests.get("http://httpbin.org/cookies")
  print(res.text)
  '''
  {
    "cookies": {}
  }
  '''
  ```

  两次请求不是同一个会话，因此不成功

+ 成功的例子(用Session对象发起多次请求)

  ```python
  import requests
  
  s = requests.Session()
  s.get("http://httpbin.org/cookies/set/number/123456789")
  res = s.get("http://httpbin.org/cookies")
  print(res.text)
  '''
  {
    "cookies": {
      "number": "123456789"
    }
  }
  '''
  ```

  > 如果要做登陆验证的话，用Session先发起POST请求，再登陆，会维持这个会话信息，就可以正常获取需要登录才能访问的页面，cookies也是自动处理的（不需要写Handler）
  >
  > Session对象发起一项项的请求，这就模拟了在浏览器里边发起的每一次请求，维持了一个登录会话。

#### 证书验证

```python
import requests

res = requests.get("https://www.12306.cn")
print(res.text)
```

> 在2018年初的时候，12306还没有证书，此段代码会 有 SSLError错误

```python
import requests

res = requests.get("https://www.12306.cn", verify=False)
print(res.status_code)
```

> 返回正常，但会有一些警告出现
>
> InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
>
> 可用下面代码消除警告

```python
import requests
import urllib3

urllib3.disable_warnings()
res = requests.get("https://www.12306.cn", verify=False)
print(res.status_code)
```

还可以手动指定证书

```python
import requests

res = requests.get("https://www.12306.cn", cert=('/path/xx.crt', '/path/key'))
print(res.status_code)
```

这里没有证书，不运行了

#### 代理设置

+ 一般情况

  ```python
  import requests
  
  proxies = {
      "http": "http://127.0.0.1:8087",
      "https": "https://127.0.0.1:8087",
  }
  
  res = requests.get("https://www.taobao.com", proxies=proxies)
  print(res.status_code)
  ```

+ 需要用户名和密码的情况

  ```python
  proxies = {
      "http": "http://user:password@127.0.0.1:8087/",
  }
  ```

+ 其他类型代理，如socks

  ```powershell
  pip3 install 'requests[socks]' -i xxx
  ```

  ```python
  proxies = {
      "http": "socks5://127.0.0.1:8087",
      "https": "socks5://127.0.0.1:8087",
  }
  ```

#### 超时设置

```python
import requests

res = requests.get("https://github.com", timeout=0.1)
print(res.status_code)
# 抛出ReadTimeout异常
```

```python
import requests
from requests.exceptions import ReadTimeout
try:
    res = requests.get("https://github.com", timeout=0.1)
    print(res.status_code)
except ReadTimeout as e:
    print(e)
```

#### 认证设置

```python
from requests.auth import HTTPBasicAuth
auth=HTTPBasicAuth('user', '123')
```

或

```python
auth=('user', '123')
```

#### 异常处理

```python
import requests
from requests.exceptions import ReadTimeout, HTTPError, RequestException
try:
    res = requests.get("https://github.com", timeout=0.1)
    print(res.status_code)
except ReadTimeout:
    print("Timeout")
except HTTPError:
    print("HTTP Error")
except RequestException:
    print("Error")
```

如果网络有问题，会抛出ConnectionError，可被RequestException捕获

## 四、正则表达式

处理字符串非常强大的工具，有自己特定的语法结构，实现字符串的检索、替换、匹配验证都不在话下。把HTML代码抓下来后，用正则表达式提取里边的信息就非常方便。

定义：正则表达式 是 对字符串操作的一种逻辑公式，就是事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。

### 引子1

有些图片的链接没有隐藏在HTML代码里边，而是在js里边，js是一个变量数组的形式，比如：

https://www.toutiao.com/a6691161019248542222/

它把组图gallery赋值给一个变量，这时候一些常用的解析库，PyQuery，BeautifulSoup就不能用，这时候就需要用正则表达式把变量提取出来，再用json格式化一些，提取其中的信息。

一般的网页可以用解析库，遇到这样特殊的网页，需要用到正则表达式提取相关信息。

### 引子2

对于一些运行时候的错误日志，如果要提取其中的时间信息，就可以构造一个正则表达式的串，写一个时间的匹配模式，就可以把所有的时间提取出来了。这就是正则表达式对一些数据的筛选，清洗之类的强大操作

### Python中 re 模块

#### re.match() 

从第1个字符开始匹配，并返回第1个成功的匹配

如果第1个字符就不匹配，无法完成正常的匹配，返回None

尽量使用范匹配模式 .*? ，使用括号得到匹配目标，尽量使用非贪婪模式

+ pattern | 模式
+ string | 匹配字符串
+ re.S | 允许.匹配换行符

```python
import re

content = """<?xml version="1.0" encoding="UTF-8" standalone="no"?><?xml version="1.2" encoding="UTF-8" standalone="no"?><?xml version="1.3" encoding="UTF-8" standalone="no"?>
"""
result = re.match("<\?xml.*?\?>", content)
print(result)  # <re.Match object; span=(0, 54), match='<?xml version="1.0" encoding="UTF-8" standalone=">
print(result.group())  # <?xml version="1.0" encoding="UTF-8" standalone="no"?>
print(result.span())  # (0, 54)

```



##### 贪婪匹配

```python
import re

content = "Hello 1234567 World This is a Regex Demo"
result = re.match("^He.*(\d+).*Demo", content)
print(result)
print(result.group(1)) # 7
```

尽可能多的匹配，至少一个数字，就仅仅匹配到了7

##### 非贪婪匹配

```python
import re

content = "Hello 1234567 World This is a Regex Demo"
result = re.match("^He.*?(\d+).*Demo", content)
print(result)
print(result.group(1)) # 1234567
```

#### 返回结果 对象 

- group()  返回匹配结果
- group(n) 返回第n个 子表达式 的结果
- span()  返回匹配结果的范围

#### re.search()

扫描整个字符串，返回**第一个**成功的匹配

#### re.findall()

搜索字符串，以列表形式返回全部能匹配的子串

#### 匹配练习

```html
<div id="songs-list">
    <h2 class="title">经典老歌</h2>
    <p class="introduction">
        经典老歌列表
    </p>
    <ul id="list" class="list-group">
        <li date-view = "2">一路上有你</li>
        <li date-view = "7">
            <a href="/2.mp3" singer="任贤齐">沧海一声笑</a>
        </li>
        <li date-view = "4" class="active">
            <a href="/3.mp3" singer="齐秦">往事随风</a>
        </li>
        <li date-view = "6"><a href="/4.mp3" singer="beyond">光辉岁月</a></li>
        <li date-view = "5"><a href="/5.mp3" singer="陈慧琳">记事本</a></li>
        <li date-view = "5">
            <a href="/6.mp3" singer="邓丽君"><i class="fa fa-user"></i>但愿人长久</a>
        </li>
    </ul>
</div>
```

+ 提取 li 标签中 active 属性的 歌手信息 和 歌名 提取出来

  ```python
  result = re.search('<li.*?active.*?singer="(.*?)".*?>(.*?)</a>', content, re.S)
  print(result) # 齐秦
print(result.group(1), result.group(2)) # 往事随风
  ```
  
  注：这个正则匹配到的字符串是，而不是齐秦的这个 li ，这里只关心最后提取的数据！！
  
  ```html
          <li date-view = "2">一路上有你</li>
          <li date-view = "7">
              <a href="/2.mp3" singer="任贤齐">沧海一声笑</a>
          </li>
          <li date-view = "4" class="active">
              <a href="/3.mp3" singer="齐秦">往事随风</a>
  ```

+ 提取超链接，歌手，歌名

  ```python
  results = re.findall('<li.*?href="(.*?)".*?singer="(.*?)".*?>(<i.*?/i>)?(.*?)</a>', content, re.S)
  print(results)
  print(type(results))
  for item in results:
      print(item[0], item[1], item[3])
  """
  /2.mp3 任贤齐 沧海一声笑
  /3.mp3 齐秦 往事随风
  /4.mp3 beyond 光辉岁月
  /5.mp3 陈慧琳 记事本
  /6.mp3 邓丽君 但愿人长久
  """
  ```

+ 匹配所有歌名

  + <li.*?>
  + 换行问题，因为有的有换行，有的没换行因此追加 \s*
  + a 标签不确定有没有 (<a.*?>)?
  + i 标签不确定 (<i.*?/i>)?
  + 内容部分 (\w+)
  + 闭合 a 标签 (</a>)?
  + 收尾 \s*</li>

  ```python
  results = re.findall('<li.*?>\s*(<a.*?>)?(<i.*?/i>)?(.*?)(</a>)?\s*</li>', content, re.S)
  print(results)
  print(type(results))
  for item in results:
      print(item[2])
  """
  一路上有你
  沧海一声笑
  往事随风
  光辉岁月
  记事本
  但愿人长久
  """
  ```

#### re.sub()

作用：替换字符串中  **每一个匹配的子串**  后 返回 替换后的字符串

```python
def sub(pattern, repl, string, count=0, flags=0):
```

+ pattern | 模式
+ repl | 替换成的字符串
+ string | 原始文本

示例代码：

+ 去掉数字（把数字替换成空串）

```python
import re

content = "Hello 1234567 World This is a Regex Demo"
result = re.sub('\d', '', content)
print(result)
```

```
Hello  World This is a Regex Demo
```

+ 替换后 包含原字符串
  + 用 \1 来表示 分组编号，需要在正则上加括号表示一下
  + 替换字符串需要加 r 表示 非转义的原始字符串

```python
import re

content = "Hello 1234567 World This is a Regex Demo"
result = re.sub("(\d)", r'\1|', content)
print(result)
```

```
Hello 1|2|3|4|5|6|7| World This is a Regex Demo
```

+ 对匹配练习中的改进：先去掉 a标签 等判断再提取

```python
content = re.sub('<[ai].*?>|</[ai].*?>', "", content)
print(content)
results = re.findall("<li.*?>(.*?)</li>", content, re.S)
for item in results:
    # 去掉换行符
    print(item.strip())
```

#### re.compile

将正则字符串编译成正则表达式对象，以便于 后续 复用这个模式

```python
import re

content = "Hello 1234567 World This is a Regex Demo"
pattern = re.compile("^He.*(\d+).*Demo", re.S)
result = re.match(pattern, content)
print(result)
print(result.group(1)) # 7
```

#### 实战练习

```python
import requests
file = open("douban.html", "w+", encoding="utf-8")
file.write(requests.get("https://book.douban.com/").text)
file.close()
```

```python
import re
file = open("douban.html", "r+", encoding="utf-8")
content = file.read()
file.close()
results = re.findall('<li.*?cover.*?href="(.*?)".*?title="(.*?)".*?more-meta.*?author">(.*?)</span>.*?year">(.*?)</span>.*?</li>', content, re.S)
for item in results:
    print(item)
```

```python
import requests
import re
content = requests.get("https://book.douban.com/").text
results = re.findall('<li.*?cover.*?href="(.*?)".*?title="(.*?)".*?more-meta.*?author">(.*?)</span>.*?year">(.*?)</span>.*?</li>', content, re.S)
for item in results:
    print(item)
```





>正则写的时候比较复杂

## 五、BeautifulSoup

灵活又方便的网页解析器，处理高效，支持多种解析器，利用它不用编写正则表达式即可方便地实现网页信息的提取，很多情况下都可以代替正则进行网页的提取。

安装略

### 解析库

![1558010626358](./img/BeautifulSoup.png)

### 基本用法

+ prettify()  |  补全残缺的标签，把代码格式化

### 选择器

#### 标签选择器

根据标签的名字进行选择，处理速度快

| 效果         | 写法                        | 返回内容                         |
| ------------ | --------------------------- | -------------------------------- |
| 选择元素     | soup.标签                   | <class 'bs4.element.Tag'>        |
| 获取名称     | soup.标签.name              | 标签名称                         |
| 获取属性     | soup.标签.attrs['属性']     | 属性的内容                       |
|              | soup.标签['属性']           | 属性的内容                       |
| 获取内容     | soup.标签.string            | 标签中间的内容                   |
| 嵌套使用     | soup.标签1.标签2            | <class 'bs4.element.Tag'>        |
| 获取子节点   | soup.标签.contents          | 子节点构成的list                 |
|              | soup.标签.children          | 子节点的迭代器                   |
|              | soup.标签.descendants       | 所有 孙子结点的 生成器，层次遍历 |
| 获取父节点   | soup.标签.parent            | <class 'bs4.element.Tag'>        |
| 获取祖先结点 | soup.标签.parents           | 所有祖先结点的 生成器            |
| 获取兄弟结点 | soup.标签.next_siblings     | 下一个兄弟结点                   |
|              | soup.标签.previous_siblings | 上一个兄弟结点                   |

> <class 'bs4.element.Tag'> 打印后是对应的html代码
>
> <class 'bs4.element.NavigableString'> 标签内部的内容

不能满足解析HTML文档的需求，因为解析的时候会遇到属性

#### 标准选择器

```python
def find_all(self, name=None, attrs={}, recursive=True, text=None, limit=None, **kwargs):
```

+ name | 标签名
+ attrs | 属性
+ id
+ class_ | class是关键词，需要加下划线
+ text | 文本内容

其他find略

### CSS选择器

通过select() 直接传入CSS选择器即可完成选择，返回list，每个元素是<class 'bs4.element.Tag'>，也可以嵌套

```python
from bs4 import BeautifulSoup
soup = BeautifulSoup(html, "lxml")
for ul in soup.select('ul')
	print(ul.select('li'))
```

+ get_text() 获取内容
+ ['attr'] 获取属性

### 总结

+ 推荐使用 lxml 解析库，必要时使用html.parser
  + 必要：代码混乱情况
+ 标签选择筛选功能弱但是速度快
+ 推荐使用 find()、find_all()、查询匹配单个结果或多个结果
+ 如果对CSS选择器熟悉建议使用 select()
+ 记住常用的获取属性和文本值的方法

## 六、PyQuery

强大又灵活的网页解析库。如果你

+ 觉得正则写起来太麻烦，
+ 觉得BeautifulSoup语法太难记
+ 属性jQuery的语法

那么PyQuery就是你的绝佳选择

> 模仿 jQuery 实现解析的一个库，API可以无缝的迁移

本节用的原始html如下：

```html
<div id="container">
    <ul class="list">
        <li class="item-0">first item</li>
        <li class="item-1"><a href="link2.html"></a>second item</li>
        <li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
        <li class="item-1 active"><a href="link4.html">fourth item</a></li>
        <li class="item-0"><a href="link5.html">fifth item</a></li>
    </ul>
</div>
```

### 初始化

+ 与BeautifulSoup对象类似，可以用一个字符串 (html代码) 创建

```python
from pyquery import PyQuery as pq
html = "<div>...</div>"
doc = pq(html)
print(doc('li'))
```

+ url = xxx |  指定URL进行初始化

```python
from pyquery import PyQuery as pq
doc = pq(url="https://www.baidu.com")
print(doc('head'))
```

+ filename = xxx | 指定文件路径进行初始化

```python
from pyquery import PyQuery as pq
doc = pq(filename="demo.html")
print(doc('li'))
```

### 基本CSS选择器

```css
#container .list li
```

### 查找元素

#### 子元素

+ find()  |  所有符合条件的
+ children()  |  所有的直接子元素，可以传入css选择器

```python
from pyquery import PyQuery as pq
html = """
<div id="container">
    <ul class="list">
        <li class="item-0">first item</li>
        <li class="item-1"><a href="link2.html"></a>second item</li>
        <li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
        <li class="item-1 active"><a href="link4.html">fourth item</a></li>
        <li class="item-0"><a href="link5.html">fifth item</a></li>
    </ul>
</div>
"""
doc = pq(html)
items = doc('.list')
print(type(items))  # <class 'pyquery.pyquery.PyQuery'>
print(items)
lis = items.find('li')
print(type(lis))  # <class 'pyquery.pyquery.PyQuery'>
print(lis)
```

```python
lis = items.children('.active')
# 返回两个li标签
```

#### 父元素

+ parent()  |  获取直接父元素
+ parents()  |  获取祖先元素集合，一级级往上

```python
from pyquery import PyQuery as pq
html = """
<div class="wrap">
    <div id="container">
        <ul class="list">
            <li class="item-0">first item</li>
            <li class="item-1"><a href="link2.html"></a>second item</li>
            <li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
            <li class="item-1 active"><a href="link4.html">fourth item</a></li>
            <li class="item-0"><a href="link5.html">fifth item</a></li>
        </ul>
    </div>
</div>
"""
doc = pq(html)
items = doc('.list')
parents = items.parents()
print(parents)
"""
<div class="wrap">
...
</div><div id="container">
...
</div>
"""

print('-----------')
parent = items.parents('#container')
print(parent)
# <div id="container">...
parent = items.parents('.item')
print(parent)
# <div class="wrap">...
```

#### 兄弟元素

+ siblings() | 所有兄弟结点，不含自己，也可以传入选择器进一步筛选



### 遍历

#### 单个元素

直接打印即可

#### 多个元素

+ items()  |  返回<class 'generator'>

```python
doc = pq(html)
lis = doc('li').items()
print(type(lis))
for li in lis:
    print(li)
```

```
<class 'generator'>
# 5个li标签
```



### 获取信息

查找到元素后，经常用到的操作就是，提取文本，代码之类的

#### 获取属性

以 href 属性为例：

+ attr('href')
+ attr.href

#### 获取文本

+ text() | 获取被包含的文本

#### 获取HTML

+ html() | 获取内层的html代码



### DOM操作(修改HTML)

#### addClass、removeClass

添加，移除class

#### attr、css

修改属性、css样式--style

#### remove

移除

#### 其他DOM方法

https://pyquery.readthedocs.io/en/latest/api.html



### 伪类选择器

```css
li:first-child
li:last-child
li:nth-child(2)
li:gt(2)  /* 获取第2个以后的 */
li:nth-child(2n)
li:contains('second') /* 根据文本选择 */
```

更多可以查看：

http://www.w3school.com.cn/cssref/css_selectors.asp

https://developer.mozilla.org/zh-CN/docs/Learn/CSS/Introduction_to_CSS/Selectors

### 官方文档

https://pyquery.readthedocs.io/



## 七、Selenium

自动化测试工具，支持多种浏览器。Safari、Chrome、Firefox、PhantomJS、Android浏览器

驱动浏览器，发一些指令，让浏览器执行，指令有：跳转，输入，点击，下拉等

爬虫中主要用来解决 JavaScript 渲染的问题

网页是后来的 js 渲染的，用这个库可以完成渲染（模拟了浏览器的整个操作），获取的source就是渲染后的

### 基本使用

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait

browser = webdriver.Chrome()
try:
    browser.get("https://www.baidu.com")
    # 输入框的 id 是kw
    input = browser.find_element_by_id('kw')
    # 向元素中 敲入 Python
    input.send_keys('Python')
    input.send_keys(Keys.ENTER)
    # 等待
    wait = WebDriverWait(browser, 10)
    # 直到某个元素加载出来
    wait.until(EC.presence_of_element_located((By.ID, 'content_left')))
    print(browser.current_url)
    print(browser.get_cookies())
    print(browser.page_source)
finally:
    browser.close()
```

### 声明浏览器对象

```python
from selenium import webdriver

browser = webdriver.Chrome()
browser = webdriver.Firefox()
browser = webdriver.Edge()
browser = webdriver.PhantomJS()
browser = webdriver.Safari()
```

### 访问页面

```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.get("https://www.taobao.com")
print(browser.page_source)
browser.close()
```

### 查找元素

现在已经把一个网页打开了，然后可能需要对网页进行一些交互的操作，比如找到输入框输入信息，找到按钮点击等等。首先看如何获取到这个元素

#### 单个元素

+ find_element_by_id()
+ find_element_by_xpath()
+ find_element_by_link_text()
+ find_element_by_partial_link_text()
+ find_element_by_tag_name()
+ find_element_by_class_name()
+ find_element_by_css_selector()

以上方法可以归结为以下通用方法：

```python
def find_element(self, by=By.ID, value=None)
```

```python
from selenium import webdriver
from selenium.webdriver.common.by import By

browser = webdriver.Chrome()
browser.get("https://www.taobao.com")
input_first = browser.find_element(By.ID, 'q')
print(input_first)
browser.close()
```

```python
<selenium.webdriver.remote.webelement.WebElement (session="73a1421eff334911028338043a730026", element="0.03692810146681924-1")>
```

#### 多个元素

把查找单个元素的element --> elements

返回 list 

```python
from selenium import webdriver
from selenium.webdriver.common.by import By

browser = webdriver.Chrome()
browser.get("https://www.taobao.com")
# lis = browser.find_elements_by_css_selector('.service-bd li')
lis = browser.find_elements(By.CSS_SELECTOR, '.service-bd li')
print(lis)
browser.close()
```

```python
[<selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-1")>, <selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-2")>, <selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-3")>, <selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-4")>, <selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-5")>, <selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-6")>, <selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-7")>, <selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-8")>, <selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-9")>, <selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-10")>, <selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-11")>, <selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-12")>, <selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-13")>, <selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-14")>, <selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-15")>, <selenium.webdriver.remote.webelement.WebElement (session="eeaf53c0a603bbfea57fb69d7c2a218f", element="0.42193460247771775-16")>]
```

### 元素交互操作

对获取的元素，调用交互方法

```python
from selenium import webdriver
import time
browser = webdriver.Chrome()
browser.get("https://www.taobao.com")
input_search = browser.find_element_by_id('q')
input_search.send_keys("iPhone")
time.sleep(1)
input_search.clear()
input_search.send_keys("iPad")
button = browser.find_element_by_class_name('btn-search')
button.click()
```

<https://selenium-python.readthedocs.io/api.html>

7.21. Remote WebDriver WebElement

https://seleniumhq.github.io/selenium/docs/api/py/api.html

### 交互动作

将动作附加到动作链中串行执行

```python
from selenium import webdriver
from selenium.webdriver import ActionChains
browser = webdriver.Chrome()
url = "https://www.runoob.com/try/try.php?filename=jqueryui-example-droppable"
browser.get(url)
browser.switch_to.frame('iframeResult')
source = browser.find_element_by_css_selector('#draggable')
target = browser.find_element_by_css_selector('#droppable')
# 动作链对象，传入browser
actions = ActionChains(browser)
actions.drag_and_drop(source, target)
# 执行动作
actions.perform()
```

https://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains

### 执行JavaScript

一些动作可能没有提供 API ,比如进度条下拉，可以通过执行 js 代码实现

```python
from selenium import webdriver

browser = webdriver.Chrome()
url = "https://www.zhihu.com/explore"
browser.get(url)
js1 = "window.scrollTo(0, document.body.scrollHeight)"
js2 = "alert('To Bottom')"
browser.execute_script(js1)
browser.execute_script(js2)
```

###  获取元素信息

#### 获取属性

+ get_attribute('attr_type')

```python
ele = browser.find_element_by_class_name('111')
ele.get_attribute('class')
```

#### 获取文本值

+ text 属性

#### 获取ID、位置、标签名、大小

+ id
+ location
+ tag_name
+ size

```python
from selenium import webdriver

browser = webdriver.Chrome()
url = "https://www.zhihu.com/explore"
browser.get(url)
logo = browser.find_element_by_id('zh-top-link-logo')
print(logo)
print(logo.get_attribute('class'))
ele = browser.find_element_by_class_name('zu-top-add-question')
print(ele.text)
print(ele.id)
print(ele.location)
print(ele.tag_name)
print(ele.size)
```

```python
<selenium.webdriver.remote.webelement.WebElement (session="619d74cffee422d9fb434ae0a1b0be67", element="0.42538195244506016-1")>
zu-top-link-logo
提问
0.42538195244506016-2
{'x': 739, 'y': 7}
button
{'height': 32, 'width': 66}
```

### Frame

iframe是独立的，因此想要获取里边的元素需要先切换

+ switch_to.frame()  |  传入id切换到frame
+ switch_to.parent_frame() 

```python
from selenium import webdriver
from selenium.common.exceptions import NoSuchElementException
browser = webdriver.Chrome()
url = "https://www.runoob.com/try/try.php?filename=jqueryui-example-droppable"
browser.get(url)
browser.switch_to.frame('iframeResult')
source = browser.find_element_by_css_selector('#draggable')
print(source)
try:
    logo = browser.find_element_by_class_name('logo')
except NoSuchElementException:
    print('NO LOGO')
browser.switch_to.parent_frame()
logo = browser.find_element_by_class_name('logo')
print(logo)
print(logo.text)
```

```python
<selenium.webdriver.remote.webelement.WebElement (session="886f9a85fe147599db6bbf015274957f", element="0.8814704646994527-1")>
NO LOGO
<selenium.webdriver.remote.webelement.WebElement (session="886f9a85fe147599db6bbf015274957f", element="0.5116864795832403-2")>
RUNOOB.COM
```

### 等待

Selenium只是把框架加载出来就执行后续语句，在爬取网页的时候，可能有一些 ajax 请求会加载不完全，从而导致一些bug，需要延长时间。确保元素加载完毕后再操作。

#### 隐式等待

当使用了隐式等待执行测试的时候，如果WebDriver没有在DOM中找到元素，将继续等待，超出设定时间后则抛出 NoSuchElementException 异常，

换句话说：当 查找元素或元素 并没有立即出现的时候，隐式等待将等待一段时间再查找DOM，默认时间为0

```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.implicitly_wait(10)
url = "https://github.com/"
browser.get(url)
ele = browser.find_element_by_class_name('link-gray')
print(ele)
```

#### 显式等待

指定一个等待条件st、最长等待时间max_time，会在max_time内判断st是否成立

+ 成立：立即返回
+ 不成立：一直等待，等待到最长等待时间
  + 还没出现的话会抛出 异常
  + 出现了则最长返回

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

browser = webdriver.Chrome()
url = "https://www.taobao.com/"
browser.get(url)
# 声明对象，传入browser和最长等待时间
wait = WebDriverWait(browser, 10)
# 用until方法，传入一些等待条件

# 判断是否出现
ele = wait.until(EC.presence_of_element_located((By.ID, 'q')))
# 判断是否可点击
button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.btn-search')))
print(ele)
print("---------------------")
print(button)
```

![1558248676695](./img/wait.png)

更多详细内容：

<https://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions>

### 前进后退

+ back()
+ forward()

```python
from selenium import webdriver
import time

browser = webdriver.Chrome()
browser.get("https://www.baidu.com")
browser.get("https://www.taobao.com")
browser.get("https://t.tt")
time.sleep(1)
browser.back()
time.sleep(1)
browser.back()
time.sleep(0.5)
browser.forward()
browser.forward()
```

### Cookies

```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.get("https://www.zhihu.com/explore")
print(browser.get_cookies())
browser.add_cookie({'name': 'name', 'domain': 'www.zhihu.com', 'value': 'Taylor'})
print(browser.get_cookies())
browser.delete_all_cookies()
print(browser.get_cookies())
```

### 选项卡管理

之前是在 一个浏览器的 同一个选项卡进行操作，现在要实现 新建、切换选项卡

最简单的方法就是：执行 js 打开

还有就是通过模拟按键 Ctrl + T (不通用)

+ window_handles  |  窗口的list
+ switch_to.window(window_handles[index])  |  切换到index选项卡

```python
from selenium import webdriver
import time

browser = webdriver.Chrome()
browser.get("https://www.zhihu.com/explore")
browser.execute_script('window.open()')
print(browser.window_handles)
browser.switch_to.window(browser.window_handles[1])
browser.get("https://www.taobao.com")
time.sleep(1)
browser.switch_to.window(browser.window_handles[0])
browser.get("https://python.org")
```

```python
['CDwindow-FE18A68AE91E6A9DE61E349D79A5A5C9', 'CDwindow-69C89213F74DB937A9D3A0E6012D0C9E']
```



### 异常处理

相对较麻烦，浏览器会出现各类异常，最好查看 API 文档

<https://selenium-python.readthedocs.io/api.html#module-selenium.common.exceptions>

+ TimeoutException
+ NoSuchElementException