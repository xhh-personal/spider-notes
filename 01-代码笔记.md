## 爬虫笔记记录

## 一、基本的爬虫

### 访问百度主页

```python
import requests

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36"}

res = requests.get("http://www.baidu.com", headers = headers)
# print(res.text)
print(res.headers) # 返回请求头
print(res.status_code) # 返回状态码
```

### 爬取百度logo

+ 坑点：Windows下文件目录！！！

```python
import requests
res = requests.get("https://www.baidu.com/img/bd_logo1.png")
with open("new.png", "wb") as f:
    f.write(res.content)
    f.close()
```

### 解决JavaScript渲染问题

+ 分析Ajax请求
+ 调用浏览器渲染Selenium/ WebDriver
+ Splash
+ PyV8、Ghost.py

### 怎么样保存数据

+ 文本方式：纯文本、json、xml等
+ 关系型数据库
+ 非关系型数据库
+ 二进制文件

## 二、Urllib库

Python内置的HTTP请求库，只需要关心请求的参数，不需要关心底层实现

内容包括：

+ urllib.request 请求模块
+ urllib.error 异常处理模块
+ urllib.parse 解析模块
+ urllib.robotparser robots.txt解析模块

函数定义：

```python
def urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,
            *, cafile=None, capath=None, cadefault=False, context=None):
```

+ url
+ data
  + 用于post传参
+ timeout
+ CA证书相关参数

示例代码：

### urlopen参数的使用*

#### get 方式

```python
import urllib.request
import urllib.parse

res = urllib.request.urlopen("http://localhost:3000/db_lab")
# print(res.read()) # 原始二进制数据
print(res.read().decode("utf-8")) # 解析后的 数据
```

#### post方式

```python
import urllib.request
import urllib.parse

data = bytes(urllib.parse.urlencode({'word': 'hello_world'}), encoding='utf=8')
res = urllib.request.urlopen("http://httpbin.org/post", data=data)
print(res.read())
```

#### timeout参数 && 捕获异常

```python
import urllib.request
import urllib.error
import socket

try:
    res = urllib.request.urlopen("http://httpbin.org/get", timeout=0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):
        print("Time Out")
else:
    print(res.read())
```

### HTTPResponse对象属性*

+ type(res)	类型是<class 'http.client.HTTPResponse'>
+ status 状态码
+ getheaders() 获取响应头，元组 组成 的列表
+ getheaders('key') 获取响应头的特定字段
+ read().decode("utf-8") 响应体解码

### urlopen 传入 Request对象

更加方便的指定 一些请求的参数，形成整体的结构

```python
import urllib.request
import urllib.parse

url = "http://httpbin.org/post"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36",
    "Host": "httpbin.org"
}
dict = {
    "name": "Amy"
}
data = bytes(urllib.parse.urlencode(dict), encoding='utf8')
req = urllib.request.Request(url=url, data=data, headers=headers, method="POST")
res = urllib.request.urlopen(req)
print(res.read().decode("utf8"))
```

```json
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "name": "Amy"
  }, 
  "headers": {
    "Accept-Encoding": "identity", 
    "Content-Length": "8", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36"
  }, 
  "json": null, 
  "origin": "211.143.85.88, 211.143.85.88", 
  "url": "https://httpbin.org/post"
}
```

另外添加 headers 的方式

```python
req.add_header('key','value')
```

以上是最基本的HTTP请求，这也对应了 request 的四个重要组成部分：url，方式，请求头，请求体(Form Data)把参数直接传递给Request对象，可以构造出整个 请求作为整体发送给 服务器。这样就可以完成大部分网站的爬取。还有一些高级用法，比如设置Cookie，设置代理这样的操作。需要一些Handler实现功能

查看urllib.request的官方文档

### Handler**

#### 代理

可以伪装自己ip地址，服务器识别IP为代理的IP，爬虫的时候可以切换IP，就有效避免 反扒

```python
import urllib.request

# 构造Handler，根据API把参数传入
# 这里传入代理的网址（XX-net之类的端口，必须存在）
proxy_hander = urllib.request.ProxyHandler({
    'http': 'http://127.0.0.1:8087',
    'https': 'https://127.0.0.1:8087'
})

opener = urllib.request.build_opener(proxy_hander)
res = opener.open('http://www.baidu.com')
```

注：未测试成功

#### Cookie

cookie可以维持登录信息，这样就可以爬取一些需要登录认证的网页

+ 获取cookie并打印出来

```python
import http.cookiejar
import urllib.request

cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
res = opener.open("http://www.baidu.com")
for item in cookie:
    print(item.name, '=', item.value)
```

```
BAIDUID = 5E2096C64DB7A23B6B5961E0DE8F4CD9:FG=1
BIDUPSID = 5E2096C64DB7A23B6B5961E0DE8F4CD9
H_PS_PSSID = 26522_1452_28777_21123_28518_28771_28722_28964_28839_28585_26350_22157
PSTM = 1557836206
delPer = 0
BDSVRTM = 0
BD_HOME = 0
```

+ 把cookie保存到文件 用http.cookiejar.MozillaCookieJar 类对象，有save方法

```python
import http.cookiejar
import urllib.request

filename = "cookie.txt"
cookie = http.cookiejar.MozillaCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
res = opener.open("http://www.baidu.com")
cookie.save(ignore_discard=True, ignore_expires=True)
```

最后txt以特定格式保存了cookie信息

> http.cookiejar.LWPCookieJar也可以

可用load方法读取cookie，再用Handler构建出来，这样再次请求的时候就会把Cookie附着进去

```python
cookie.load("cookie.txt",ignore_discard=True, ignore_expires=True)
```

如果网页需要登录，有了cookie后，请求的结果就是登录后所能看到的

### 异常处理**

#### URLError

```python
from urllib import request
from urllib import error
try:
    res = request.urlopen("http://localhost:3000/pad")
except error.URLError as e:
    print(e.reason)
```

经测试，不同url返回如下：

```
http://localhost:3000/pad  Not Found
http://o.o  [Errno 11001] getaddrinfo failed
http://localhost  [WinError 10061] 由于目标计算机积极拒绝，无法连接。
```

#### HTTPError

先捕捉HTTPError再捕捉URLError是比较好的写法：

```python
from urllib import request
from urllib import error
try:
    res = request.urlopen("http://localhost:3000/f")
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, sep='\n')
except error.URLError as e:
    print(e.reason)
else:
    print("Request Successful!")
```

```
Not Found
404
Date: Tue, 14 May 2019 12:36:34 GMT
Server: Apache/2.4.25 (Win32) OpenSSL/1.0.2j PHP/5.6.30
Content-Length: 199
Connection: close
Content-Type: text/html; charset=iso-8859-1
```

#### 具体验证异常原因的方法

```python
import socket
from urllib import request
from urllib import error
try:
    res = request.urlopen("http://www.baidu.com", timeout=0.001)
except error.URLError as e:
    print(type(e))
    if isinstance(e.reason, socket.timeout):
        print("TIME OUT")
```

```
<class 'urllib.error.URLError'>
TIME OUT
```

### URL解析

#### urlparse

函数定义：

```python
def urlparse(url, scheme='', allow_fragments=True)
```

传入url，把url分割成几个部分，然后每个部分依次赋值

```python
from urllib.parse import urlparse
result = urlparse("https://baike.baidu.com/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91/85895?fr=aladdin#3_2") # <class 'urllib.parse.ParseResult'>
print(type(result), result)
'''
ParseResult(scheme='https', netloc='baike.baidu.com', path='/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91/85895', params='', query='fr=aladdin', fragment='3_2')
'''
```

第二个参数scheme

+ 若URL中有协议类型，此参数失效
+ 若URL中无协议类型，此参数会填充到返回结果

第三个参数allow_fragments

+ False情况 -- 把锚点解析到query或者path中去，不会解析到fragment中
+ True情况 -- 正常情况

#### urlunparse

作用：将URL进行拼接

```python
from urllib.parse import urlunparse

data = ["http", "www.baidu.com","index.html",'user','a=6','comment']
print(urlunparse(data))
```

```
http://www.baidu.com/index.html;user?a=6#comment
```

#### urljoin

作用：用于拼接URL，两个URL拼接成一个新的URL；规则：URL可以分成6个字段，以后边的URL为基准，如果某些字段不存在，会用前边的URL来补充

![1557886678927](./img/urljoin.png)

#### urlencode

作用：把一个字典对象转化为get请求参数

```python
from urllib.parse import urlencode

params = {
    "name": "xiaohuihui",
    "age": 22
}
base_url = "http://www.baidu.com?"
url = base_url + urlencode(params)
print(url)
```

```
http://www.baidu.com?name=xiaohuihui&age=22
```

>urllib中最重要的是urllib.request模块，提供了很多的Handler
>
><https://docs.python.org/3/library/urllib.html>

> urllib库使用中有很多不方便的地方，不如加代理，处理Cookie的时候，操作繁琐，另外发送POST请求也……

## 三、Request库

比urllib库更好用，节约大量工作，完全满足HTTP测试需求 

Python实现的简单易用的HTTP库

https://2.python-requests.org/en/master/

### 实例引入

```python
import requests
res = requests.get("http://59.67.225.73/jiaowuchu")
print(type(res)) # <class 'requests.models.Response'>
print(res.status_code) # 200
print(type(res.text)) # <class 'str'>
print(res.text) # html代码
print(res.cookies) # <RequestsCookieJar[]>
```

+ status_code
+ text
+ cookie

### 各种请求方式

```python
import requests
r1 = requests.get(url="",params="")
r1 = requests.post(url="", data="", json="")
r1 = requests.put(url="",data="")
r1 = requests.delete(url="")
r1 = requests.head(url="")
r1 = requests.options(url="")
```

后边都是不定长关键词参数**kwargs

#### 基本get请求

##### 基本写法

```python
import requests

res = requests.get("http://httpbin.org/get")
print(res.text)
```

##### 带参数的get请求

+ 手动加入?xx=xx
+ 字典形式加入，更方便的构造get请求的URL

```python
import requests

data = {
    "name": "Amy",
    "age": 18
}
res = requests.get("http://httpbin.org/get", params=data)
print(res.text)
```

```json
{
  "args": {
    "age": "18", 
    "name": "Amy"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.21.0"
  }, 
  "origin": "222.222.191.99, 222.222.191.99", 
  "url": "https://httpbin.org/get?name=Amy&age=18"
}
```

##### 解析json

在一些ajax请求中比较常用

```python
import requests
import json

data = {
    "name": "Amy",
    "age": 18
}
res = requests.get("http://httpbin.org/get", params=data)
print(type(res.text)) # <class 'str'>
print(res.text)
print(type(res.json())) # <class 'dict'>
print(json.loads(res.text)) # res.json()是实际效果
```

##### 获取二进制数据content

下载图片、视频等常用的方法

```python
import requests

res = requests.get("http://59.67.225.73/jiaowuchu/images/ncepu60.png")
print(type(res.text), type(res.content))
print(res.text)  # <class 'str'> 一堆乱码
print(res.content)  # <class 'bytes'> 字节流
with open("ncepu60.png", "wb") as f:
    f.write(res.content)
    f.close()
```

##### 添加headers

有些网站会识别User-Agent，否则会被拒绝

```python
import requests

res = requests.get("https://www.zhihu.com/explore")
print(res.text)
```

<html>

<head><title>400 Bad Request</title></head>
<body bgcolor="white">

<center><h1>400 Bad Request</h1></center>
<hr><center>openresty</center>
</body>
</html>

添加User-Agent后，相当于进行了浏览器的伪装，就正常访问了

```python
import requests

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0"
}
res = requests.get("https://www.zhihu.com/explore", headers = headers)
with open("result.html", "w+", encoding="utf-8") as f:
    f.write(res.text)
    f.close()
```

#### POST请求

定义：

```python
def post(url, data=None, json=None, **kwargs)
```

可以另外加headers

```python
import requests

data = {
    'c': "网络1502"
}
res = requests.post("http://localhost:3000/GPA/getdata.php", data=data)
print(res.json())
```

附PHP后台代码：

```php
<?php
define('IN_TG', true);
require dirname(__FILE__).'/includes/common.inc.php';
function getdata($sql){
    $result = mysql_query($sql);
    $json = array();
    while (!!$row =mysql_fetch_array($result,MYSQL_ASSOC)) {
        array_push($json,$row);
    }
    echo json_encode($json);
}
if(isset($_POST['a'])){
    $zyh = $_POST['a'];
    getdata("SELECT sno,GM_BJ.bm,sname,xfj FROM XFJ,GM_BJ WHERE XFJ.BM = GM_BJ.BM AND ZYH = $zyh ORDER BY SNO;");
}
if(isset($_POST['b'])){
    $zyh = $_POST['b'];
    getdata("SELECT bm FROM gm_bj WHERE zyh=$zyh ORDER BY bm");
}
if(isset($_POST['c'])){
    $bm = $_POST['c'];
    getdata("SELECT sno,bm,sname,xfj FROM XFJ WHERE bm = '$bm' ORDER BY SNO");
}
?>
```

#### 响应

##### response属性

+ status | int类型，状态码
+ headers | <class 'requests.structures.CaseInsensitiveDict'> 请求头字典
+ cookies | <class 'requests.cookies.RequestsCookieJar'> 
+ url | <class 'str'>
+ history | <class 'list'> 访问的历史记录
+ encoding、next、ok、reason、

#### 状态码判断

<code>res.status_code ==  request.codes.【关键词】</code>

<code>res.status_code == 状态码</code>

```python
import requests

res = requests.get("http://www.baidu.com")
exit() if not res.status_code == requests.codes.ok else print("Request Successfully")
```

```python
import requests

res = requests.get("http://www.baidu.com")
exit() if not res.status_code == 200 else print("Request Successfully")
```

```python
_codes = {

    # Informational.
    100: ('continue',),
    101: ('switching_protocols',),
    102: ('processing',),
    103: ('checkpoint',),
    122: ('uri_too_long', 'request_uri_too_long'),
    200: ('ok', 'okay', 'all_ok', 'all_okay', 'all_good', '\\o/', '✓'),
    201: ('created',),
    202: ('accepted',),
    203: ('non_authoritative_info', 'non_authoritative_information'),
    204: ('no_content',),
    205: ('reset_content', 'reset'),
    206: ('partial_content', 'partial'),
    207: ('multi_status', 'multiple_status', 'multi_stati', 'multiple_stati'),
    208: ('already_reported',),
    226: ('im_used',),

    # Redirection.
    300: ('multiple_choices',),
    301: ('moved_permanently', 'moved', '\\o-'),
    302: ('found',),
    303: ('see_other', 'other'),
    304: ('not_modified',),
    305: ('use_proxy',),
    306: ('switch_proxy',),
    307: ('temporary_redirect', 'temporary_moved', 'temporary'),
    308: ('permanent_redirect',
          'resume_incomplete', 'resume',),  # These 2 to be removed in 3.0

    # Client Error.
    400: ('bad_request', 'bad'),
    401: ('unauthorized',),
    402: ('payment_required', 'payment'),
    403: ('forbidden',),
    404: ('not_found', '-o-'),
    405: ('method_not_allowed', 'not_allowed'),
    406: ('not_acceptable',),
    407: ('proxy_authentication_required', 'proxy_auth', 'proxy_authentication'),
    408: ('request_timeout', 'timeout'),
    409: ('conflict',),
    410: ('gone',),
    411: ('length_required',),
    412: ('precondition_failed', 'precondition'),
    413: ('request_entity_too_large',),
    414: ('request_uri_too_large',),
    415: ('unsupported_media_type', 'unsupported_media', 'media_type'),
    416: ('requested_range_not_satisfiable', 'requested_range', 'range_not_satisfiable'),
    417: ('expectation_failed',),
    418: ('im_a_teapot', 'teapot', 'i_am_a_teapot'),
    421: ('misdirected_request',),
    422: ('unprocessable_entity', 'unprocessable'),
    423: ('locked',),
    424: ('failed_dependency', 'dependency'),
    425: ('unordered_collection', 'unordered'),
    426: ('upgrade_required', 'upgrade'),
    428: ('precondition_required', 'precondition'),
    429: ('too_many_requests', 'too_many'),
    431: ('header_fields_too_large', 'fields_too_large'),
    444: ('no_response', 'none'),
    449: ('retry_with', 'retry'),
    450: ('blocked_by_windows_parental_controls', 'parental_controls'),
    451: ('unavailable_for_legal_reasons', 'legal_reasons'),
    499: ('client_closed_request',),

    # Server Error.
    500: ('internal_server_error', 'server_error', '/o\\', '✗'),
    501: ('not_implemented',),
    502: ('bad_gateway',),
    503: ('service_unavailable', 'unavailable'),
    504: ('gateway_timeout',),
    505: ('http_version_not_supported', 'http_version'),
    506: ('variant_also_negotiates',),
    507: ('insufficient_storage',),
    509: ('bandwidth_limit_exceeded', 'bandwidth'),
    510: ('not_extended',),
    511: ('network_authentication_required', 'network_auth', 'network_authentication'),
}
```

### 高级操作

request函数可以携带的参数汇总：

+ url
+ param | get参数
+ headers 
+ data | post参数
+ files | POST文件上传
+ verify | 是否验证
+ cert | 手动导入证书
+ proxies | 代理设置
+ timeout | 超时设置
+ auth | 认证设置

#### 文件上传

```python
import requests

files = {
    'file0': open("ncepu60.png", "rb"),
    'file1': open("new.png", "rb")
}
res = requests.post("http://httpbin.org/post", files=files)
print(res.text)
```

输出结果：

```python
{
  "args": {}, 
  "data": "", 
  "files": {
    "file0": "data:application/octet-stream;base64,……", 
    "file1": "data:application/octet-stream;base64,……"
  }, 
  "form": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "61023", 
    "Content-Type": "multipart/form-data; boundary=b1ecaa54edacb0ce921c2b6cb01adece", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.21.0"
  }, 
  "json": null, 
  "origin": "222.222.191.99, 222.222.191.99", 
  "url": "https://httpbin.org/post"
}
```

#### 获取cookie

```python
import requests

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/17.17134"
}
res = requests.get("https://www.baidu.com", headers=headers)
print(res.cookies)
print()
for k, v in res.cookies.items():
    print(k, '=', v)

```

#### 会话维持

模拟登陆

+ 不成功的例子

  ```python
  import requests
  
  requests.get("http://httpbin.org/cookies/set/number/123456789")
  res = requests.get("http://httpbin.org/cookies")
  print(res.text)
  '''
  {
    "cookies": {}
  }
  '''
  ```

  两次请求不是同一个会话，因此不成功

+ 成功的例子(用Session对象发起多次请求)

  ```python
  import requests
  
  s = requests.Session()
  s.get("http://httpbin.org/cookies/set/number/123456789")
  res = s.get("http://httpbin.org/cookies")
  print(res.text)
  '''
  {
    "cookies": {
      "number": "123456789"
    }
  }
  '''
  ```

  > 如果要做登陆验证的话，用Session先发起POST请求，再登陆，会维持这个会话信息，就可以正常获取需要登录才能访问的页面，cookies也是自动处理的（不需要写Handler）
  >
  > Session对象发起一项项的请求，这就模拟了在浏览器里边发起的每一次请求，维持了一个登录会话。

#### 证书验证

```python
import requests

res = requests.get("https://www.12306.cn")
print(res.text)
```

> 在2018年初的时候，12306还没有证书，此段代码会 有 SSLError错误

```python
import requests

res = requests.get("https://www.12306.cn", verify=False)
print(res.status_code)
```

> 返回正常，但会有一些警告出现
>
> InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
>
> 可用下面代码消除警告

```python
import requests
import urllib3

urllib3.disable_warnings()
res = requests.get("https://www.12306.cn", verify=False)
print(res.status_code)
```

还可以手动指定证书

```python
import requests

res = requests.get("https://www.12306.cn", cert=('/path/xx.crt', '/path/key'))
print(res.status_code)
```

这里没有证书，不运行了

#### 代理设置

+ 一般情况

  ```python
  import requests
  
  proxies = {
      "http": "http://127.0.0.1:8087",
      "https": "https://127.0.0.1:8087",
  }
  
  res = requests.get("https://www.taobao.com", proxies=proxies)
  print(res.status_code)
  ```

+ 需要用户名和密码的情况

  ```python
  proxies = {
      "http": "http://user:password@127.0.0.1:8087/",
  }
  ```

+ 其他类型代理，如socks

  ```powershell
  pip3 install 'requests[socks]' -i xxx
  ```

  ```python
  proxies = {
      "http": "socks5://127.0.0.1:8087",
      "https": "socks5://127.0.0.1:8087",
  }
  ```

#### 超时设置

```python
import requests

res = requests.get("https://github.com", timeout=0.1)
print(res.status_code)
# 抛出ReadTimeout异常
```

```python
import requests
from requests.exceptions import ReadTimeout
try:
    res = requests.get("https://github.com", timeout=0.1)
    print(res.status_code)
except ReadTimeout as e:
    print(e)
```

#### 认证设置

```python
from requests.auth import HTTPBasicAuth
auth=HTTPBasicAuth('user', '123')
```

或

```python
auth=('user', '123')
```

#### 异常处理

```python
import requests
from requests.exceptions import ReadTimeout, HTTPError, RequestException
try:
    res = requests.get("https://github.com", timeout=0.1)
    print(res.status_code)
except ReadTimeout:
    print("Timeout")
except HTTPError:
    print("HTTP Error")
except RequestException:
    print("Error")
```

如果网络有问题，会抛出ConnectionError，可被RequestException捕获

## 四、正则表达式

### 引子1

有些图片的链接没有隐藏在HTML代码里边，而是在js里边，js是一个变量数组的形式，比如：

https://www.toutiao.com/a6691161019248542222/

它把组图gallery赋值给一个变量，这时候一些常用的解析库，PyQuery，BeautifulSoup就不能用，这时候就需要用正则表达式把变量提取出来，再用json格式化一些，提取其中的信息。

一般的网页可以用解析库，遇到这样特殊的网页，需要用到正则表达式提取相关信息。

### 引子2

对于一些运行时候的错误日志，如果要提取其中的时间信息，就可以构造一个正则表达式的串，写一个时间的匹配模式，就可以把所有的时间提取出来了。这就是正则表达式对一些数据的筛选，清洗之类的强大操作

### Python中 re 模块





>正则写的时候比较复杂

## 五、BeautifulSoup

灵活又方便的网页解析器，处理高效，支持多种解析器，利用它不用编写正则表达式即可方便地实现网页信息的提取，很多情况下都可以代替正则进行网页的提取。

安装略

### 解析库

![1558010626358](C:\Users\73654\Documents\Study_Folder\Test\Python爬虫学习笔记\img\BeautifulSoup.png)

### 基本用法

```python
from bs4 import BeautifulSoup

html = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <link rel="stylesheet" href="./css/basic.css">
    <title>Document</title>
    <style>
        #register{
            text-align: center;
        }
    </style>
</head>
<body>
    
    <div id="register">
        <h2>注册帐号</h2><br>
        <span>请认真填写以下内容:</span><br>
        用 户 名：<input type="text" name="UserID" id="UserID" class="inputstyle" style="margin-top:37px"><br>
        密  码：<input type="password" name="UserPD" id="UserPD" class="inputstyle"><br>
        确认密码：<input type="password" name="UserPD" id="UserPD" class="inputstyle"><br>
        年  龄：<input type="text" name="UserID" id="UserID" class="inputstyle"><br>
        <input type="submit" value="注册" id="submit_button">
"""

soup = BeautifulSoup(html, "lxml")
print(soup.prettify()) # 补全残缺的标签，把代码格式化
print(soup.title.string) # 选择了title，打印出内容
```





